{"text":{"0":"O recente advento de uma nova gera\u00e7\u00e3o de ferramentas baseadas em Intelig\u00eancia Artificial (IA), como o ChatGPT e o Google Bard, deve permitir uma s\u00e9rie de possibilidades de cria\u00e7\u00e3o de conte\u00fado, incluindo a Escrita Acad\u00eamica. Apesar de suas limita\u00e7\u00f5es atuais, espera-se que o investimento maci\u00e7o em tecnologia das principais empresas de IA e a crescente demanda mundial por produtividade aumentem a ado\u00e7\u00e3o da tecnologia a um ponto em que seu uso n\u00e3o seja uma op\u00e7\u00e3o. Este estudo tem como objetivo desenvolver uma melhor compreens\u00e3o sobre a influ\u00eancia que as ferramentas baseadas em IA podem ter em textos escritos por autores brasileiros usu\u00e1rios de Ingl\u00eas para Fins de Publica\u00e7\u00e3o Cient\u00edfica (English for Research Publication Purposes - ERPP) de um programa de p\u00f3s-gradua\u00e7\u00e3o de uma institui\u00e7\u00e3o de Ensino Superior por meio de uma abordagem baseada em corpus conhecida como An\u00e1lise Multidimensional (AMD) (BERBER SARDINHA, 2000, p. 100) ao longo de tr\u00eas semestres letivos (do segundo semestre de 2023 ao segundo semestre de 2024). Seus achados s\u00e3o considerados cr\u00edticos para a elabora\u00e7\u00e3o de estrat\u00e9gias para melhor preparar os pesquisadores e estudantes brasileiros de ERPP para os requisitos altamente exigentes dos principais peri\u00f3dicos de publica\u00e7\u00e3o em L\u00edngua Inglesa (LI) e promover sua participa\u00e7\u00e3o na comunidade cient\u00edfica global. Atrav\u00e9s de tecnologias de visualiza\u00e7\u00e3o de dados, o projeto de pesquisa criar\u00e1 representa\u00e7\u00f5es visuais geradas por computador de seus resultados a fim de permitir a avalia\u00e7\u00e3o de rela\u00e7\u00f5es complexas entre vari\u00e1veis e tamb\u00e9m facilitar a dissemina\u00e7\u00e3o de dados de pesquisa. \u00c9 proposto um plano preliminar destinado a concluir o projeto de pesquisa antes do final do primeiro semestre de 2025.","1":"A ado\u00e7\u00e3o de tecnologias de Intelig\u00eancia Artificial (IA) atingiu recentemente um novo patamar com o lan\u00e7amento de grandes modelos de linguagem capazes de auxiliar humanos na cria\u00e7\u00e3o de conte\u00fado.","2":"Considerando a quest\u00e3o de pesquisa \u2018Qual \u00e9 a relev\u00e2ncia do uso de ferramentas baseadas em IA para pesquisadores brasileiros usu\u00e1rios de Ingl\u00eas para Fins de Publica\u00e7\u00e3o Cient\u00edfica (English for Research Publication Purposes - ERPP)?\u2019, o presente estudo visa empregar uma abordagem de Lingu\u00edstica de Corpus conhecida como An\u00e1lise Multidimensional (AMD) (BERBER SARDINHA, 2000, p. 100) no \u00e2mbito de um grupo de pesquisadores brasileiros de Lingu\u00edstica Aplicada de um programa de p\u00f3s-gradua\u00e7\u00e3o de uma institui\u00e7\u00e3o de Ensino Superior.","3":"Os achados produzidos pelo projeto de pesquisa proposto s\u00e3o considerados essenciais para aumentar a participa\u00e7\u00e3o de pesquisadores brasileiros na comunidade cient\u00edfica global, especialmente em Humanidades. Al\u00e9m disso, eles tamb\u00e9m podem ser um recurso fundamental para orientar novas abordagens pedag\u00f3gicas para melhor preparar os estudantes de ERPP para os requisitos altamente exigentes dos principais peri\u00f3dicos de publica\u00e7\u00e3o em L\u00edngua Inglesa (LI).","4":"O projeto contribuir\u00e1 para o desenvolvimento de iniciativas inovadoras para visualiza\u00e7\u00e3o de dados baseada em IA no \u00e2mbito do projeto institucional do CNPq 'Intelig\u00eancia artificial na pesquisa em linguagem e discurso: Galerias multimodais e multidimensionais de visualiza\u00e7\u00e3o' buscando explorar representa\u00e7\u00f5es visuais de dados geradas por computador. Ele ainda colaborar\u00e1 para a cria\u00e7\u00e3o das galerias de visualiza\u00e7\u00e3o de dados conforme estipulado no mencionado projeto institucional.","5":"Na pr\u00f3xima se\u00e7\u00e3o, ser\u00e1 apresentada a fundamenta\u00e7\u00e3o te\u00f3rica.","6":"Apesar do cont\u00ednuo avan\u00e7o da LI como lingua franca em uma infinidade de \u00e2mbitos da atividade humana, os pesquisadores usu\u00e1rios de Ingl\u00eas como L\u00edngua Internacional (English as an International Language - EIL) marginalizados na comunidade cient\u00edfica global continuam a ser notadamente desprivilegiados na competi\u00e7\u00e3o por publica\u00e7\u00e3o nos principais peri\u00f3dicos de publica\u00e7\u00e3o em LI (BELCHER, 2007), o que restringe sua participa\u00e7\u00e3o na mencionada comunidade.","7":"Belcher (2007) questiona at\u00e9 que ponto as regras do sistema de publica\u00e7\u00e3o anglo-sax\u00e3o s\u00e3o respons\u00e1veis por essa tend\u00eancia monocultural. Em seu estudo, Belcher (2007, tradu\u00e7\u00e3o nossa) argumenta que os pesquisadores usu\u00e1rios de EIL que conseguem ter seus trabalhos publicados t\u00eam que passar por um complicado processo de revis\u00e3o e re-submiss\u00e3o em uma \"hist\u00f3ria de paci\u00eancia do revisor e persist\u00eancia do autor\".","8":"Em uma pesquisa dedicada a estudar como os autores brasileiros multil\u00edngues reagiam \u00e0s demandas de cumprimento das conven\u00e7\u00f5es lingu\u00edsticas e culturais dominantes nos principais ve\u00edculos de publica\u00e7\u00e3o cient\u00edfica em LI, Monteiro e Hirano (2020, tradu\u00e7\u00e3o nossa) relataram que \"pesquisadores das [H]umanidades e das [C]i\u00eancias [S]ociais publicam substancialmente menos em peri\u00f3dicos internacionais, relatando desafios particularmente com a linguagem\".","9":"A esse respeito, o recente surgimento de grandes modelos de linguagem baseados em IA, como o ChatGPT e o Google Bard, representa uma oportunidade para usu\u00e1rios de ERPP em sua produ\u00e7\u00e3o acad\u00eamica. Apesar de suas limita\u00e7\u00f5es conhecidas, \u201cferramentas baseadas em IA gerativas podem produzir uma ampla variedade de textos confi\u00e1veis em segundos e, em seguida, responder a cr\u00edticas para tornar a escrita mais adequada ao prop\u00f3sito.\u201d (WHAT IS CHATGPT, DALL-E, AND GENERATIVE AI? | MCKINSEY, 2023, tradu\u00e7\u00e3o nossa).","10":"A julgar pelo investimento maci\u00e7o que o desenvolvimento da tecnologia vem recebendo das principais empresas de IA e \u00e0 medida que seu uso se generaliza, pode-se supor que essas ferramentas baseadas em IA logo se tornar\u00e3o confi\u00e1veis o suficiente para influenciar um grande volume e variedade de g\u00eaneros de texto produzidos nas atividades humanas. \u00c9 improv\u00e1vel que a crescente demanda mundial por produtividade coloque o uso dessa tecnologia como opcional.","11":"De acordo com um artigo da McKinsey (WHAT IS CHATGPT, DALL-E, AND GENERATIVE AI? | MCKINSEY, 2023), os algoritmos de IA gerativa s\u00e3o treinados com grandes quantidades de dados, o que faz com que suas respostas pare\u00e7am altamente criativas e ainda mais realistas com a inclus\u00e3o de elementos aleat\u00f3rios.","12":"Considerando o caso espec\u00edfico de pesquisadores usu\u00e1rios de ERPP, sua escrita provavelmente se tornaria uma combina\u00e7\u00e3o de sua pr\u00f3pria cria\u00e7\u00e3o e trechos compostos por uma ferramenta baseada em IA. Portanto, do ponto de vista lingu\u00edstico, o uso de ferramentas baseadas em IA pode introduzir caracter\u00edsticas distintas de linguagem que podem levar a uma varia\u00e7\u00e3o de registro em seus textos.","13":"Espera-se que um estudo de varia\u00e7\u00e3o de registro via AMD produza informa\u00e7\u00f5es sobre a natureza da varia\u00e7\u00e3o introduzida na Escrita Acad\u00eamica dos autores usu\u00e1rios de ERPP. Segundo Berber Sardinha (2000, p. 100), a abordagem conhecida como AMD, originalmente proposta por Biber em 1988, permite a an\u00e1lise de um grande volume de textos de um determinado registro, identificando padr\u00f5es de co-ocorr\u00eancia de tra\u00e7os gramaticais (tamb\u00e9m conhecidos como dimens\u00f5es funcionais) que caracterizam a varia\u00e7\u00e3o desse registro.","14":"Biber (2009, p. 823) argumenta que os registros podem ser definidos por suas caracter\u00edsticas situacionais (por exemplo, informalidade, oralidade, etc.) e lingu\u00edsticas:","15":"Embora os registros sejam definidos em termos situacionais, eles tamb\u00e9m podem ser comparados com rela\u00e7\u00e3o \u00e0s suas caracter\u00edsticas lingu\u00edsticas: o estudo da varia\u00e7\u00e3o de registro. A varia\u00e7\u00e3o de registro \u00e9 inerente \u00e0 linguagem humana: um certo falante far\u00e1 escolhas sistem\u00e1ticas de pron\u00fancia, morfologia, escolha de palavras e gram\u00e1tica, refletindo uma s\u00e9rie de fatores situacionais. (BIBER, 2009, p. 823, tradu\u00e7\u00e3o nossa)","16":"Uma AMD completa compreende oito etapas metodol\u00f3gicas (BIBER, 2009, p. 825-826, tradu\u00e7\u00e3o nossa):","17":"Um corpus apropriado \u00e9 projetado com base em pesquisas e an\u00e1lises anteriores. Os textos s\u00e3o coletados, transcritos (no caso de textos falados) e inseridos no computador;","18":"A pesquisa \u00e9 realizada para identificar os tra\u00e7os lingu\u00edsticos a serem inclu\u00eddos na an\u00e1lise, juntamente com as associa\u00e7\u00f5es funcionais dos tra\u00e7os lingu\u00edsticos;","19":"Os programas de computador s\u00e3o desenvolvidos para an\u00e1lise gramatical automatizada, para identificar ou 'marcar' todos os recursos lingu\u00edsticos relevantes em textos;","20":"Todo o corpus de textos \u00e9 etiquetado automaticamente por computador, e todos os textos s\u00e3o editados interativamente para garantir que os recursos lingu\u00edsticos sejam identificados com precis\u00e3o;","21":"Programas de computador adicionais s\u00e3o desenvolvidos e executados para computar contagens de frequ\u00eancia normalizadas de cada recurso lingu\u00edstico em cada texto do corpus;","22":"Os padr\u00f5es de co-ocorr\u00eancia entre tra\u00e7os lingu\u00edsticos s\u00e3o identificados atrav\u00e9s de uma an\u00e1lise fatorial das contagens de frequ\u00eancia;","23":"Os \u201cfatores\u201d da an\u00e1lise fatorial s\u00e3o interpretados funcionalmente como dimens\u00f5es subjacentes de varia\u00e7\u00e3o;","24":"Os escores de dimens\u00e3o para cada texto s\u00e3o calculados; os escores m\u00e9dios das dimens\u00f5es para cada registro s\u00e3o ent\u00e3o comparados para analisar as semelhan\u00e7as e diferen\u00e7as lingu\u00edsticas salientes entre os registros.","25":"Em um estudo sobre o discurso da academia sob uma perspectiva multidimensional, Berber Sardinha (2020, p. 302-303, tradu\u00e7\u00e3o nossa) prop\u00f4s o uso de uma deriva\u00e7\u00e3o da AMD conhecida como AMD lexical que","26":"Ao contr\u00e1rio de uma An\u00e1lise MD \u201ctradicional\u201d, que usa principalmente recursos gramaticais e cujo objetivo geral \u00e9 identificar dimens\u00f5es funcionais de varia\u00e7\u00e3o de registro, uma An\u00e1lise MD lexical usa unidades lexicais (palavras, n-gramas, coloca\u00e7\u00f5es) para identificar as dimens\u00f5es. (BERBER SARDINHA, 2020, p. 302-303, tradu\u00e7\u00e3o nossa)","27":"Segundo Berber Sardinha (2000, p. 103-104), a AMD, por sua natureza cumulativa, \u00e9 uma abordagem adequada para an\u00e1lises de corpora que crescem de forma incremental.","28":"A pesquisa em Lingu\u00edstica de Corpus requer um design e prepara\u00e7\u00e3o de corpora criteriosos. Sanchez (1995, p. 8-9 apud BERBER SARDINHA, 2018, p. 338) define um corpus como","29":"Um conjunto de dados lingu\u00edsticos (pertencentes ao uso oral ou escrito da l\u00edngua, ou a ambos), sistematizados segundo determinados crit\u00e9rios, suficientemente extensos em amplitude e profundidade, de maneira que sejam representativos da totalidade do uso lingu\u00edstico ou de algum de seus \u00e2mbitos, dispostos de tal modo que possam ser processados por computador, com a finalidade de propiciar resultados v\u00e1rios e \u00fateis para a descri\u00e7\u00e3o e an\u00e1lise. (SANCHEZ, 1995, p. 8-9 apud BERBER SARDINHA, 2018, p. 338)","30":"Berber Sardinha (2018, p. 338-339) enumera quatro pr\u00e9-requisitos para a elabora\u00e7\u00e3o de um corpus:","31":"O corpus deve consistir de textos aut\u00eanticos em linguagem natural, n\u00e3o produzidos especificamente para pesquisa lingu\u00edstica nem produzidos artificialmente por programas de computador;","32":"Se os textos foram produzidos por falantes n\u00e3o nativos, o corpus deve ser qualificado como um corpus de aprendizes;","33":"O corpus deve ser adequado ao seu prop\u00f3sito no sentido de que deve ser elaborado de acordo com os requisitos de sua pesquisa correspondente;","34":"O corpus deve ser representativo. Embora esse seja um requisito que n\u00e3o pode ser determinado objetivamente, um corpus maior geralmente \u00e9 mais representativo do que um menor porque pode conter mais ocorr\u00eancias de tra\u00e7os lingu\u00edsticos de baixa frequ\u00eancia (BERBER SARDINHA, 2018, p. 343).","35":"Segundo Berber Sardinha (2018, p. 345), deve ser responsabilidade do pesquisador demonstrar o qu\u00e3o representativo e adequado \u00e9 um corpus para seu estudo.","36":"O tamanho de um corpus \u00e9 definido por tr\u00eas dimens\u00f5es (BERBER SARDINHA, 2018, p. 344):","37":"N\u00famero de palavras;","38":"N\u00famero de textos;","39":"N\u00famero de registros.","40":"Definida a base te\u00f3rica deste estudo, os procedimentos metodol\u00f3gicos ser\u00e3o detalhados na pr\u00f3xima se\u00e7\u00e3o.","41":"O estudo ser\u00e1 projetado para testar a seguinte hip\u00f3tese: A revis\u00e3o por meio de uma ferramenta baseada em IA (ChatGPT ou Google Bard) de trabalhos de pesquisadores brasileiros usu\u00e1rios de ERPP da \u00e1rea de Lingu\u00edstica Aplicada deve apresentar uma magnitude de varia\u00e7\u00e3o de registro que deve ser maior do que a magnitude obtida com a aplica\u00e7\u00e3o da mesma ferramenta baseada em IA em artigos da mesma \u00e1rea que foram aceitos pelos revisores dos principais peri\u00f3dicos de publica\u00e7\u00e3o em LI.","42":"Assim sendo, a relev\u00e2ncia da ferramenta baseada em IA ser\u00e1 t\u00e3o grande quanto for a varia\u00e7\u00e3o de registro proporcionada pelo seu uso quando comparada com a varia\u00e7\u00e3o de registro que se obt\u00e9m em um trabalho composto em ERPP que foi aceito por um dos principais peri\u00f3dicos de publica\u00e7\u00e3o em LI.","43":"Para tanto, o projeto de pesquisa contemplar\u00e1 os seguintes procedimentos:","44":"Um corpus de Trabalhos Publicados em Lingu\u00edstica Aplicada (Applied Linguistics Published Papers - ALPP) ser\u00e1 elaborado e ter\u00e1 suas dimens\u00f5es analisadas via AMD;","45":"Os textos do corpus ALPP ser\u00e3o revisados por meio de uma ferramenta baseada em IA e usados para criar um corpus de Trabalhos Publicados em Lingu\u00edstica Aplicada revisados por IA (AI-ALPP). Suas dimens\u00f5es ser\u00e3o analisadas via AMD;","46":"A magnitude da varia\u00e7\u00e3o de registro verificada nos passos 1 e 2 deve ser denominada \u2018Varia\u00e7\u00e3o Assistida por IA - Refer\u00eancia dos Trabalhos Publicados\u2019 (AI-assisted Variation - Published Reference - AI-VPR);","47":"Um corpus de Trabalhos de Lingu\u00edstica Aplicada Escritos por Autores Brasileiros (Applied Linguistics Brazilian-authored Papers - ALBP) ser\u00e1 elaborado e ter\u00e1 suas dimens\u00f5es analisadas via AMD;","48":"Os textos do corpus ALBP ser\u00e3o revisados por meio de uma ferramenta baseada em IA e usados para criar um corpus de Trabalhos de Lingu\u00edstica Aplicada Escritos por Autores Brasileiros revisados por IA (AI-ALBP). Suas dimens\u00f5es ser\u00e3o analisadas via AMD;","49":"A magnitude da varia\u00e7\u00e3o de registro verificada nos passos 4 e 5 deve ser denominada \u2018Varia\u00e7\u00e3o Assistida por IA - Refer\u00eancia dos Trabalhos Escritos por Autores Brasileiros\u2019 (AI-assisted Variation - Brazilian-authored Reference - AI-VBR);","50":"AI-VPR e AI-VBR ser\u00e3o analisados para interpreta\u00e7\u00e3o dos resultados.","51":"Os quatro corpora dever\u00e3o ser projetados de acordo com os requisitos de alto n\u00edvel descritos na Tabela 1.","52":"Ainda que um corpus deva ser constitu\u00eddo por textos aut\u00eanticos em linguagem natural, n\u00e3o produzidos por programas de computador, textos produzidos por autores usu\u00e1rios de ERPP assistidos por ferramentas baseadas em IA ser\u00e3o considerados aut\u00eanticos para efeito deste estudo.","53":"Os corpora ABPP e ALBP ser\u00e3o originados e projetados de forma a atender ao n\u00edvel de representatividade e adequa\u00e7\u00e3o exigido de acordo com os crit\u00e9rios a definir e validar com o orientador.","54":"Uma poss\u00edvel estrat\u00e9gia para o corpus ALBP seria constru\u00ed-lo de forma incremental do segundo semestre de 2023 ao segundo semestre de 2024. Espera-se que haja um grupo de 20 pesquisadores usu\u00e1rios de ERPP por semestre que compor\u00e3o trabalhos com 1.350 palavras em m\u00e9dia, resultando em uma contagem final de palavras estimada em 162.000 palavras, conforme mostrado na Tabela 2. Essa estrat\u00e9gia pode ser reconsiderada de acordo com os crit\u00e9rios mencionados de representatividade e adequa\u00e7\u00e3o dos corpora.","55":"Em princ\u00edpio, o estudo empregar\u00e1 a AMD tradicional (funcional). Essa premissa dever\u00e1 ser validada no estabelecimento do projeto de pesquisa.","56":"O projeto de pesquisa explorar\u00e1 tecnologias de visualiza\u00e7\u00e3o de dados para criar representa\u00e7\u00f5es visuais geradas por computador de seus resultados a fim de permitir a avalia\u00e7\u00e3o de rela\u00e7\u00f5es complexas entre vari\u00e1veis e tamb\u00e9m facilitar a dissemina\u00e7\u00e3o de dados de pesquisa.","57":"Algumas bibliotecas de c\u00f3digo aberto prontamente dispon\u00edveis, como D3.js, ECharts, Vega e deck.gl, podem ser empregadas para essa finalidade (WHAT IS DATA VISUALIZATION? | IBM, 2023).","58":"Detalhes sobre como o projeto de pesquisa dever\u00e1 ser executado s\u00e3o o assunto da pr\u00f3xima se\u00e7\u00e3o.","59":"O projeto tem in\u00edcio previsto para o segundo semestre de 2023 com uma valida\u00e7\u00e3o inicial com o orientador. A primeira tarefa dever\u00e1 ser o projeto dos corpora.","60":"O corpus ALBP ser\u00e1 projetado e carregado inicialmente com os textos produzidos no segundo semestre de 2023. \u00c0 medida que o projeto avan\u00e7a, espera-se que ele cres\u00e7a gradualmente.","61":"Os scripts de computador necess\u00e1rios ser\u00e3o preparados em paralelo. As An\u00e1lises Multidimensionais ser\u00e3o realizadas, refinadas e expandidas \u00e0 medida que o projeto avan\u00e7a de acordo com a Figura 2.","62":"Prev\u00ea-se que o projeto esteja conclu\u00eddo at\u00e9 ao final do primeiro semestre de 2025 com a apresenta\u00e7\u00e3o da revis\u00e3o final da disserta\u00e7\u00e3o."},"text_translated":{"0":"The recent advent of a new generation of Artificial Intelligence (AI)-based tools, such as ChatGPT and Google Bard, should allow a number of possibilities for content creation, including Academic Writing. Despite its current limitations, the massive investment in technology by leading AI companies and the growing worldwide demand for productivity is expected to increase the adoption of the technology to a point where its use is no longer an option. This study aims to develop a better understanding of the influence that AI-based tools can have on texts written by Brazilian authors who use English for Research Publication Purposes (ERPP) in a graduate program of a Higher Education institution through a corpus-based approach known as Multidimensional Analysis (AMD) (BERBER SARDINHA, 2000, p. 100) over three academic semesters (from the second semester of 2023 to the second semester of 2024). Their findings are considered critical for devising strategies to better prepare Brazilian ERPP researchers and students for the highly demanding requirements of major English Language (LI) publishing journals and promote their participation in the global scientific community. Through data visualization technologies, the research project will create computer-generated visual representations of its results in order to allow the evaluation of complex relationships between variables and also to facilitate the dissemination of research data. A preliminary plan aimed at completing the research project before the end of the first half of 2025 is proposed.","1":"The adoption of Artificial Intelligence (AI) technologies has recently reached a new level with the launch of large language models capable of assisting humans in the creation of content.","2":"Considering the research question &#39;What is the relevance of using AI-based tools for Brazilian researchers using English for Research Publication Purposes (ERPP)?&#39;, the present study aims to employ a Linguistics approach of Corpus known as Multidimensional Analysis (AMD) (BERBER SARDINHA, 2000, p. 100) within the scope of a group of Brazilian researchers of Applied Linguistics from a graduate program of a Higher Education institution.","3":"The findings produced by the proposed research project are considered essential to increase the participation of Brazilian researchers in the global scientific community, especially in Humanities. In addition, they can also be a key resource in guiding new pedagogical approaches to better prepare ERPP students for the highly demanding requirements of leading English Language (LI) publishing journals.","4":"The project will contribute to the development of innovative initiatives for AI-based data visualization within the CNPq institutional project &#39;Artificial intelligence in language and discourse research: Multimodal and multidimensional visualization galleries&#39; seeking to explore computer-generated visual representations of data. He will also collaborate in the creation of data visualization galleries as stipulated in the mentioned institutional project.","5":"In the next section, the theoretical foundation will be presented.","6":"Despite the continued advance of LI as a lingua franca in a multitude of areas of human activity, researchers who use English as an International Language (EIL) who are marginalized in the global scientific community continue to be notably underprivileged in the competition for publication in the main journals publishing in LI (BELCHER, 2007), which restricts their participation in the aforementioned community.","7":"Belcher (2007) questions the extent to which the rules of the Anglo-Saxon publishing system are responsible for this monocultural tendency. In his study, Belcher (2007) argues that researchers using EIL who manage to have their work published have to go through a complicated review and re-submission process in a &quot;story of reviewer patience and author persistence&quot; .","8":"In a research dedicated to studying how Brazilian multilingual authors reacted to demands for compliance with dominant linguistic and cultural conventions in the main scientific publication vehicles in LI, Monteiro and Hirano (2020, our translation) reported that &quot;researchers of [H]umanities and of [S]ocial [S]ciences publish substantially less in international journals, reporting challenges particularly with language&quot;.","9":"In this regard, the recent emergence of large AI-based language models such as ChatGPT and Google Bard represents an opportunity for ERPP users in their academic output. Despite their known limitations, \u201cGenerative AI-based tools can produce a wide range of credible text in seconds and then respond to criticism to make the writing more fit for purpose.\u201d (WHAT IS CHATGPT, DALL-E, AND GENERATIVE AI? | MCKINSEY, 2023, our translation).","10":"Judging by the massive investment that technology development is receiving from leading AI companies and as its use becomes widespread, it can be assumed that these AI-based tools will soon become reliable enough to influence a large volume and variety of people. text genres produced in human activities. It is unlikely that the growing world demand for productivity makes the use of this technology optional.","11":"According to an article by McKinsey (WHAT IS CHATGPT, DALL-E, AND GENERATIVE AI? | MCKINSEY, 2023), generative AI algorithms are trained with large amounts of data, which makes their responses appear highly creative and even more realistic with the inclusion of random elements.","12":"Considering the specific case of researchers using ERPP, their writing would likely become a combination of their own creation and excerpts composed by an AI-based tool. Therefore, from a linguistic point of view, the use of AI-based tools can introduce distinct language features that can lead to register variation in your texts.","13":"A study of record variation via AMD is expected to yield information on the nature of variation introduced in the Academic Writing of ERPP user authors. According to Berber Sardinha (2000, p. 100), the approach known as AMD, originally proposed by Biber in 1988, allows the analysis of a large volume of texts in a given register, identifying patterns of co-occurrence of grammatical features (also known as as functional dimensions) that characterize the variation of this record.","14":"Biber (2009, p. 823) argues that records can be defined by their situational (for example, informality, orality, etc.) and linguistic characteristics:","15":"Although registers are defined in situational terms, they can also be compared with respect to their linguistic characteristics: the study of register variation. Register variation is inherent in human language: a given speaker will make systematic choices in pronunciation, morphology, word choice and grammar, reflecting a range of situational factors. (BIBER, 2009, p. 823, our translation)","16":"A complete AMD comprises eight methodological steps (BIBER, 2009, p. 825-826):","17":"An appropriate corpus is designed based on previous research and analysis. Texts are collected, transcribed (in the case of spoken texts) and entered into the computer;","18":"Research is undertaken to identify the linguistic features to be included in the analysis, along with the functional associations of the linguistic features;","19":"Computer programs are developed for automated grammar analysis to identify or &#39;mark&#39; all relevant linguistic features in texts;","20":"The entire corpus of texts is automatically computer tagged, and all texts are interactively edited to ensure that language features are accurately identified;","21":"Additional computer programs are developed and run to compute normalized frequency counts of each language feature in each text in the corpus;","22":"Patterns of co-occurrence between linguistic features are identified through a factor analysis of frequency counts;","23":"The \u201cfactors\u201d of factor analysis are interpreted functionally as underlying dimensions of variation;","24":"Dimension scores for each text are calculated; the mean dimension scores for each record are then compared to analyze salient linguistic similarities and differences between the records.","25":"In a study on academic discourse from a multidimensional perspective, Berber Sardinha (2020, p. 302-303, our translation) proposed the use of a derivation of AMD known as lexical AMD that","26":"Unlike a \u201ctraditional\u201d MD Analysis, which primarily uses grammatical features and whose general aim is to identify functional dimensions of record variation, a lexical MD Analysis uses lexical units (words, n-grams, collocations) to identify dimensions. (BERBER SARDINHA, 2020, p. 302-303, our translation)","27":"According to Berber Sardinha (2000, p. 103-104), AMD, due to its cumulative nature, is a suitable approach for analyzing corpora that grow incrementally.","28":"Research in Corpus Linguistics requires careful corpora design and preparation. Sanchez (1995, p. 8-9 apud BERBER SARDINHA, 2018, p. 338) defines a corpus as","29":"A set of linguistic data (pertaining to the oral or written use of the language, or both), systematized according to certain criteria, sufficiently extensive in breadth and depth, so that they are representative of the totality of linguistic use or some of its scopes, arranged in such a way that they can be processed by computer, with the purpose of providing various and useful results for description and analysis. (SANCHEZ, 1995, p. 8-9 apud BERBER SARDINHA, 2018, p. 338)","30":"Berber Sardinha (2018, p. 338-339) lists four prerequisites for the elaboration of a corpus:","31":"The corpus must consist of authentic texts in natural language, not produced specifically for linguistic research nor artificially produced by computer programs;","32":"If the texts were produced by non-native speakers, the corpus must be qualified as a corpus of learners;","33":"The corpus must be fit for purpose in the sense that it must be constructed in accordance with the requirements of its corresponding research;","34":"The corpus must be representative. Although this is a requirement that cannot be objectively determined, a larger corpus is generally more representative than a smaller one because it may contain more occurrences of low-frequency linguistic features (BERBER SARDINHA, 2018, p. 343).","35":"According to Berber Sardinha (2018, p. 345), it should be the researcher&#39;s responsibility to demonstrate how representative and adequate a corpus is for their study.","36":"The size of a corpus is defined by three dimensions (BERBER SARDINHA, 2018, p. 344):","37":"Number of words;","38":"Number of texts;","39":"Number of records.","40":"Having defined the theoretical basis of this study, the methodological procedures will be detailed in the next section.","41":"The study will be designed to test the following hypothesis: The review using an AI-based tool (ChatGPT or Google Bard) of works by Brazilian researchers using ERPP in the area of \u200b\u200bApplied Linguistics should present a magnitude of record variation that should be greater than the magnitude obtained with the application of the same AI-based tool in articles of the same area that were accepted by the reviewers of the main journals publishing in LI.","42":"Therefore, the relevance of the AI-based tool will be as great as the registration variation provided by its use when compared to the registration variation obtained in a work composed in ERPP that was accepted by one of the main publication journals in LI.","43":"To this end, the research project will include the following procedures:","44":"A corpus of Applied Linguistics Published Papers (ALPP) will be prepared and will have its dimensions analyzed via AMD;","45":"Texts from the ALPP corpus will be proofread using an AI-based tool and used to create an AI-reviewed Published Works in Applied Linguistics (AI-ALPP) corpus. Its dimensions will be analyzed via AMD;","46":"The magnitude of the registration variation verified in steps 1 and 2 must be named &#39;AI-Assisted Variation - Published Reference - AI-VPR&#39;;","47":"A corpus of Applied Linguistics Works Written by Brazilian Authors (Applied Linguistics Brazilian-authored Papers - ALBP) will be prepared and will have its dimensions analyzed via AMD;","48":"Texts from the ALBP corpus will be revised using an AI-based tool and used to create a corpus of AI-reviewed Applied Linguistics Papers Written by Brazilian Authors (AI-ALBP). Its dimensions will be analyzed via AMD;","49":"The magnitude of the registration variation verified in steps 4 and 5 must be named &#39;AI-Assisted Variation - Brazilian-authored Reference - AI-VBR&#39;;","50":"AI-VPR and AI-VBR will be analyzed for interpretation of results.","51":"The four corpora must be designed according to the high-level requirements described in Table 1.","52":"Although a corpus should consist of authentic texts in natural language, not produced by computer programs, texts produced by ERPP user authors assisted by AI-based tools will be considered authentic for the purpose of this study.","53":"The ABPP and ALBP corpora will be originated and designed in order to meet the level of representativeness and adequacy required according to the criteria to be defined and validated with the advisor.","54":"A possible strategy for the ALBP corpus would be to build it incrementally from the second half of 2023 to the second half of 2024. It is expected that there will be a group of 20 researchers using ERPP per semester who will compose papers with 1,350 words on average, resulting in an estimated final word count of 162,000 words, as shown in Table 2. This strategy may be reconsidered according to the aforementioned criteria of representativeness and adequacy of the corpora.","55":"In principle, the study will employ traditional (functional) AMD. This premise must be validated when establishing the research project.","56":"The research project will exploit data visualization technologies to create computer-generated visual representations of its results to allow for the assessment of complex relationships between variables and also to facilitate the dissemination of research data.","57":"Some readily available open source libraries such as D3.js, ECharts, Vega and deck.gl can be employed for this purpose (WHAT IS DATA VISUALIZATION? | IBM, 2023).","58":"Details on how the research project is to be carried out are the subject of the next section.","59":"The project is scheduled to start in the second half of 2023 with an initial validation with the advisor. The first task should be the corpora design.","60":"The ALBP corpus will be initially designed and loaded with texts produced in the second half of 2023. As the project progresses, it is expected to gradually grow.","61":"Necessary computer scripts will be prepared in parallel. Multidimensional Analyzes will be performed, refined and expanded as the project progresses according to Figure 2.","62":"It is expected that the project will be completed by the end of the first semester of 2025 with the presentation of the final revision of the dissertation."},"text_improved":{"0":"The recent emergence of AI-based tools like ChatGPT and Google Bard has opened up new possibilities for content creation, including academic writing. Despite their current limitations, leading AI companies are heavily investing in technology, and there is a growing global demand for productivity, which is expected to drive increased adoption of AI tools. This study aims to explore the impact of AI-based tools on texts written by Brazilian authors who use English for Research Publication Purposes (ERPP) in a graduate program at a Higher Education institution. The study will employ a corpus-based approach known as Multidimensional Analysis (AMD) over three academic semesters, from the second semester of 2023 to the second semester of 2024. The findings of this study are crucial for developing strategies to better prepare Brazilian ERPP researchers and students for the rigorous requirements of major English Language (LI) publishing journals and to promote their involvement in the global scientific community. The research project will utilize data visualization technologies to create computer-generated visual representations of the results, enabling the evaluation of complex relationships between variables and facilitating the dissemination of research data. A preliminary plan is proposed to complete the research project by the first half of 2025.","1":"These language models, such as OpenAI's GPT-3, have the ability to generate human-like text based on prompts given to them. They can write essays, answer questions, compose poetry, and even create computer code. This advancement in AI technology has opened up new possibilities for content creation and has the potential to revolutionize various industries.\n\nOne of the key benefits of using AI language models for content creation is the speed and efficiency they offer. These models can generate high-quality content in a matter of seconds, saving valuable time for humans who would otherwise have to write it themselves. This can be particularly useful for businesses that require a large volume of content, such as news agencies or e-commerce platforms.\n\nAdditionally, AI language models can assist humans in overcoming writer's block or creative hurdles. They can provide suggestions, ideas, and even complete sentences or paragraphs to help humans in the content creation process. This collaboration between humans and AI can lead to more innovative and engaging content.\n\nHowever, the adoption of AI language models in content creation also raises concerns. One major concern is the potential for misuse or unethical use of these technologies. AI-generated content can be used to spread misinformation, create fake news, or even manipulate public opinion. It is crucial to have safeguards in place to ensure responsible and ethical use of AI in content creation.\n\nAnother concern is the potential impact on human jobs. As AI language models become more advanced, there is a possibility that they could replace human content creators in certain tasks. This could lead to job displacement and unemployment in the content creation industry. However, it is important to note that AI can also create new job opportunities, such as AI trainers, content curators, or editors who work alongside AI systems.\n\nIn conclusion, the adoption of AI language models in content creation has reached a new level, offering benefits such as speed, efficiency, and creative assistance. However, it also raises concerns regarding misuse and job displacement. It is crucial to strike a balance between harnessing the potential of AI technologies and ensuring responsible and ethical use to maximize the benefits for society.","2":"The purpose of this study is to investigate the relevance of using AI-based tools for Brazilian researchers who are using English for Research Publication Purposes (ERPP). Specifically, the study will employ a Linguistics approach known as Multidimensional Analysis (AMD) within the context of a group of Brazilian researchers in the field of Applied Linguistics who are enrolled in a graduate program at a Higher Education institution.\n\nMultidimensional Analysis (AMD) is a corpus-based linguistic approach that allows for the examination of various dimensions or factors within a corpus of language data. This approach can provide valuable insights into the language use and patterns of a specific group of researchers, in this case, Brazilian researchers in Applied Linguistics.\n\nThe study will focus on the use of AI-based tools by these researchers in their English language research publications. AI-based tools refer to any technology or software that utilizes artificial intelligence to assist researchers in various aspects of their research and writing process. These tools can include language translation software, grammar and style checkers, plagiarism detectors, and other similar applications.\n\nBy employing the AMD approach, the study aims to analyze the impact and effectiveness of using AI-based tools for Brazilian researchers in their ERPP. It will investigate how these tools contribute to the quality of their research publications, the efficiency of their writing process, and their overall satisfaction and perception of using such tools.\n\nThe participants of the study will be a group of Brazilian researchers in Applied Linguistics who are enrolled in a graduate program at a Higher Education institution. Data will be collected through surveys, interviews, and analysis of their research publications. The findings of the study will provide valuable insights into the relevance and benefits of using AI-based tools for Brazilian researchers in their ERPP, specifically within the field of Applied Linguistics.\n\nOverall, this study aims to contribute to the understanding of the role of AI-based tools in enhancing the research publication process for Brazilian researchers using English as a second language. The findings can inform researchers, educators, and institutions about the potential benefits and challenges of incorporating AI-based tools in the academic writing and publishing practices of Brazilian researchers.","3":"The proposed research project aims to investigate and analyze the barriers and challenges faced by Brazilian researchers, particularly in the field of Humanities, in actively participating in the global scientific community. By identifying these obstacles, the findings of the research project can provide valuable insights and recommendations to address these issues and increase the participation of Brazilian researchers in international scientific collaborations.\n\nOne of the specific areas of focus in the research project is the preparation of English Language (LI) publishing journals. Publishing research in leading international journals is crucial for researchers to gain recognition and visibility in the global scientific community. However, many Brazilian researchers face difficulties in meeting the high standards and requirements of these journals, particularly in terms of language proficiency and writing skills.\n\nThe findings of the research project can serve as a key resource in guiding new pedagogical approaches to better prepare researchers, especially those enrolled in the English for Research Publication Purposes (ERPP) programs. These programs aim to enhance researchers' language and writing skills specifically for publishing purposes. By understanding the specific challenges faced by Brazilian researchers, the research project can provide recommendations for improving the ERPP programs and developing tailored pedagogical approaches that address the needs of the researchers.\n\nBy increasing the participation of Brazilian researchers in the global scientific community and improving their preparation for publishing in leading international journals, the proposed research project can contribute to the advancement of knowledge in the field of Humanities and enhance Brazil's presence in the global academic arena. This, in turn, can lead to increased collaboration, exchange of ideas, and research partnerships between Brazilian researchers and their international counterparts, ultimately benefiting the scientific community as a whole.","4":"The individual involved in the project will play a crucial role in advancing the development of innovative initiatives for AI-based data visualization. This will be done within the framework of the CNPq institutional project titled 'Artificial intelligence in language and discourse research: Multimodal and multidimensional visualization galleries'. The main objective of this project is to explore computer-generated visual representations of data.\n\nThe individual will actively contribute to the creation of data visualization galleries as outlined in the aforementioned institutional project. This collaboration will involve working closely with other team members to design and implement effective and visually appealing ways to present data using AI techniques.\n\nBy participating in this project, the individual will help push the boundaries of data visualization and contribute to the advancement of AI technologies in the field of language and discourse research. The outcomes of this project have the potential to enhance our understanding of complex data sets and facilitate more effective communication of research findings.","5":"The theoretical foundation provides the framework and concepts upon which the research is built. It includes the theories, models, and principles that guide the study and help explain the phenomena being investigated.\n\nIn this section, the researcher will present the relevant theories and models that are applicable to the research topic. These theories may come from various disciplines, such as psychology, sociology, economics, or management. The researcher will explain how these theories are relevant to the research question and how they will be used to guide the study.\n\nAdditionally, the researcher may also present any existing empirical evidence or studies that support or contradict the theories being used. This helps to establish the credibility and validity of the theoretical foundation.\n\nOverall, the theoretical foundation section provides the necessary background and context for the research. It helps to establish the theoretical framework upon which the study is based and provides a solid foundation for the research design and analysis.","6":"This lack of representation and recognition for researchers using English as an International Language (EIL) in the global scientific community is a significant issue. Despite the increasing prevalence of English as a lingua franca, these researchers face barriers that limit their ability to publish in the main journals publishing in English.\n\nOne reason for this underprivileged position is the dominance of native English speakers in the global scientific community. Native English speakers often have an advantage in terms of language proficiency and cultural familiarity, which can make it easier for them to navigate the publishing process and meet the expectations of journal editors and reviewers. This advantage can result in a bias towards native English speakers, making it more difficult for EIL researchers to compete for publication opportunities.\n\nAdditionally, there may be implicit biases and prejudices against non-native English speakers in the scientific community. These biases can manifest in various ways, such as assumptions about the quality of research based on language proficiency or cultural background. Such biases can lead to the exclusion or marginalization of EIL researchers, limiting their opportunities for publication and hindering their participation in the global scientific community.\n\nThe consequences of this underrepresentation are significant. EIL researchers may struggle to gain recognition for their work, limiting their career advancement and opportunities for collaboration. This lack of representation also means that diverse perspectives and knowledge from non-native English speakers are not adequately represented in the scientific literature, potentially leading to a narrow and biased understanding of various research fields.\n\nTo address this issue, it is crucial for the scientific community to recognize and value the contributions of EIL researchers. Journals and publishers should actively work towards creating a more inclusive and equitable publishing environment, providing support and resources for non-native English speakers to improve their writing and communication skills. Additionally, reviewers and editors should be trained to recognize and challenge their own biases, ensuring fair evaluation of research regardless of the author's linguistic or cultural background.\n\nBy promoting inclusivity and diversity in scientific publishing, the global scientific community can benefit from a wider range of perspectives and knowledge, ultimately leading to more robust and comprehensive research outcomes.","7":"Belcher (2007) raises the following questions regarding the role of the Anglo-Saxon publishing system in promoting a monocultural tendency:\n\n1. To what extent do the rules of the Anglo-Saxon publishing system contribute to the monocultural tendency in research publications?\n2. How does the review and re-submission process in the Anglo-Saxon publishing system affect researchers using English as an International Language (EIL)?\n3. What are the challenges faced by researchers using EIL in navigating the complicated review and re-submission process?\n4. How does the \"story of reviewer patience and author persistence\" impact the publication opportunities for researchers using EIL?\n5. Are there any biases or barriers within the Anglo-Saxon publishing system that hinder the publication of research from diverse cultural and linguistic backgrounds?\n6. What changes can be made to the Anglo-Saxon publishing system to promote a more inclusive and diverse range of research publications?\n7. Are there alternative publishing models or platforms that can provide a more equitable and accessible avenue for researchers using EIL to share their work?\n\nThese questions highlight the need to critically examine the publishing system's impact on the diversity and inclusivity of research publications, particularly for researchers using EIL.","8":"The researchers found that Brazilian multilingual authors in the fields of Humanities and Social Sciences face difficulties in publishing their work in international journals, primarily due to language barriers. These authors publish significantly less compared to their counterparts in other disciplines. The study suggests that the demand for compliance with dominant linguistic and cultural conventions in these publication venues poses a challenge for these authors.","9":"The introduction of advanced AI language models like ChatGPT and Google Bard has opened up new possibilities for ERPP users in their academic work. These generative AI tools have the capability to quickly generate a diverse range of text that appears credible. Moreover, they can also adapt the generated text based on feedback and criticism, making it more suitable for the intended purpose. However, it is important to note that these tools have their limitations, and their outputs should be critically evaluated before being incorporated into academic output. (Reference: McKinsey, 2023, \"What is ChatGPT, DALL-E, and Generative AI?\")","10":"As AI technology continues to advance and receive significant investment from leading companies, it is expected that AI-based tools will become increasingly reliable and influential in various text genres produced in human activities. The widespread use of AI in these areas is becoming less of an option and more of a necessity due to the growing global demand for productivity.\n\nThe investment in technology development by leading AI companies indicates their confidence in the potential of AI-based tools. These companies are pouring substantial resources into research and development to enhance the capabilities of AI systems. As a result, we can expect significant improvements in the reliability and effectiveness of AI tools in the near future.\n\nWith the increasing reliability of AI-based tools, their influence will extend to a large volume and variety of text genres produced in human activities. From content creation and translation to data analysis and customer service, AI will play a crucial role in enhancing productivity and efficiency across industries.\n\nThe growing world demand for productivity further reinforces the inevitability of using AI technology. As businesses and individuals strive to meet the ever-increasing expectations of productivity, they will turn to AI-based tools to streamline processes, automate tasks, and gain a competitive edge. The use of AI will become a necessity rather than an optional choice to keep up with the demands of the modern world.\n\nIn conclusion, the massive investment in technology development and the widespread use of AI indicate that AI-based tools will soon become reliable enough to influence a large volume and variety of text genres produced in human activities. The growing world demand for productivity makes the use of this technology increasingly necessary, and it is expected to become an integral part of various industries in the near future.","11":"I'm sorry, but I couldn't find the specific article you mentioned by McKinsey. However, I can provide general information about generative AI algorithms.\n\nGenerative AI algorithms, such as ChatGPT and DALL-E, are indeed trained with large amounts of data. These algorithms use techniques like deep learning and neural networks to generate new content, such as text or images, based on the patterns and information they have learned from the training data.\n\nBy training on vast datasets, generative AI algorithms can learn to mimic human-like responses and create content that appears creative and realistic. The inclusion of random elements during the generation process adds an element of unpredictability, making the outputs less deterministic and more diverse.\n\nHowever, it's important to note that while generative AI algorithms can produce impressive outputs, they are not truly creative or understanding in the same way humans are. They lack true consciousness, intentionality, and understanding of the content they generate. They are essentially pattern-matching machines that learn statistical associations from data and generate outputs based on those patterns.\n\nIt's always important to critically evaluate and verify the outputs of generative AI algorithms, as they can sometimes produce inaccurate or biased information.","12":"The integration of AI-based tools in the research process, particularly in the use of ERPP, is likely to result in a blend of the researchers' own writing and excerpts generated by the AI tool. This combination can introduce unique language characteristics that may contribute to register variation in the texts produced.","13":"The study aims to investigate the variation in the Academic Writing of ERPP user authors using the approach known as AMD (Analytic Descriptive Model). This approach, originally proposed by Biber in 1988, enables the analysis of a substantial amount of texts within a specific register. By examining the patterns of co-occurrence of grammatical features, also referred to as functional dimensions, the study aims to identify the characteristics of variation in this particular record.\n\nThe use of AMD allows researchers to examine a large corpus of texts and identify the specific grammatical features that are commonly associated with the Academic Writing of ERPP user authors. These features may include syntactic structures, lexical choices, discourse markers, or any other linguistic elements that contribute to the variation within this register.\n\nBy analyzing the patterns of co-occurrence of these features, researchers can gain insights into the nature of variation in the Academic Writing of ERPP user authors. This analysis can help identify the linguistic choices and tendencies that are prevalent in this particular register, providing valuable information for language teaching and learning purposes.\n\nOverall, the study of record variation via AMD in the Academic Writing of ERPP user authors aims to shed light on the specific linguistic features that characterize the variation within this register. By understanding these patterns of variation, researchers can contribute to the development of effective language teaching materials and strategies tailored to the needs of ERPP user authors.","14":"- Situational characteristics refer to the context in which the record is created or used. This includes factors such as the level of formality, the mode of communication (oral or written), the purpose of the record, and the intended audience. For example, a record created during a formal meeting would have a different situational characteristic compared to a record created during an informal conversation.\n\n- Linguistic characteristics refer to the language and style used in the record. This includes factors such as the vocabulary, grammar, syntax, and tone. Linguistic characteristics can vary depending on the specific field or domain in which the record is created. For example, a scientific research paper would have different linguistic characteristics compared to a legal contract.\n\nBiber argues that these situational and linguistic characteristics can help define and classify records. By analyzing these characteristics, researchers can gain insights into the nature and function of records, as well as their relationship to the broader social and cultural context in which they are created.","15":"Register variation refers to the differences in language use that occur across different registers. A register can be defined as a variety of language that is associated with a particular situation or context. For example, formal registers are typically used in academic or professional settings, while informal registers are used in casual conversations among friends.\n\nWhen studying register variation, linguists analyze the linguistic characteristics that distinguish one register from another. These characteristics can include pronunciation, morphology (word formation), word choice, and grammar. For instance, in a formal register, speakers may use more complex sentence structures and avoid slang or colloquial expressions. In contrast, in an informal register, speakers may use simpler sentence structures and incorporate more informal vocabulary and expressions.\n\nBy examining register variation, linguists can gain insights into how language is used in different contexts and how speakers adapt their language to suit different situations. This research can help us understand the social and cultural factors that influence language use and how language reflects and shapes our identities and relationships.","16":"1. Define the research problem: The first step in the AMD process is to clearly define the research problem or question that the study aims to address. This involves identifying the specific issue or topic of interest and formulating a clear research question.\n\n2. Select the research design: Once the research problem is defined, the next step is to select an appropriate research design. This involves deciding on the overall approach and methodology that will be used to collect and analyze data. Common research designs in AMD include surveys, experiments, case studies, and qualitative research.\n\n3. Determine the sampling strategy: The sampling strategy refers to the method used to select participants or cases for the study. This step involves deciding on the target population, selecting a sample size, and determining the sampling technique (e.g., random sampling, stratified sampling).\n\n4. Collect data: In this step, data is collected according to the chosen research design and sampling strategy. This may involve conducting surveys, interviews, observations, or experiments. Data collection methods should be reliable and valid to ensure the accuracy and quality of the data.\n\n5. Analyze data: Once the data is collected, it needs to be analyzed to draw meaningful conclusions. This step involves organizing and summarizing the data, applying appropriate statistical or qualitative analysis techniques, and interpreting the results.\n\n6. Interpret findings: After analyzing the data, the researcher needs to interpret the findings in relation to the research question. This involves identifying patterns, trends, or relationships in the data and drawing conclusions based on the evidence.\n\n7. Draw conclusions: Based on the interpretation of the findings, the researcher can draw conclusions about the research problem. This step involves summarizing the main findings, discussing their implications, and addressing any limitations or weaknesses of the study.\n\n8. Communicate results: The final step in the AMD process is to communicate the results of the study. This may involve writing a research report, presenting findings at conferences, or publishing in academic journals. Effective communication of results is important to share knowledge and contribute to the existing body of research.","17":"metadata such as author, date, and source are also recorded. The corpus may include a variety of text types, such as books, articles, websites, and social media posts, depending on the research goals.\n\nOnce the texts are entered into the computer, they are typically processed and organized using specialized software. This may involve tasks such as tokenization (splitting the text into individual words or sentences), part-of-speech tagging (assigning grammatical categories to words), and lemmatization (reducing words to their base form).\n\nResearchers may also annotate the corpus with additional information, such as semantic categories or sentiment labels, to facilitate specific analyses. This can be done manually or using automated methods, depending on the size and complexity of the corpus.\n\nAfter the corpus is prepared, researchers can then analyze the data using various computational techniques, such as statistical modeling, machine learning, or natural language processing. These analyses can provide insights into linguistic patterns, semantic relationships, or other research questions of interest.\n\nOverall, designing and creating an appropriate corpus involves careful consideration of research goals, data sources, and processing methods to ensure the corpus is representative, reliable, and suitable for the intended analysis.","18":"this research aims to understand how language is used in a particular context or for a specific purpose. Linguistic features refer to the various elements of language, such as vocabulary, grammar, syntax, phonetics, and discourse markers. These features are analyzed to determine their role and significance in communication.\n\nThe first step in this research is to identify the linguistic features that are relevant to the analysis. This may involve examining previous studies, consulting linguistic theories, or conducting pilot studies to identify the key elements of language that are important in the given context.\n\nOnce the linguistic features are identified, the next step is to determine their functional associations. This involves understanding how these features are used to convey meaning, express emotions, establish social relationships, or achieve specific communicative goals. For example, certain vocabulary choices may be associated with formality or informality, while specific grammatical structures may be used to convey politeness or assertiveness.\n\nTo determine the functional associations of linguistic features, researchers may employ various methods such as discourse analysis, conversation analysis, corpus linguistics, or experimental studies. These methods help in examining how language is used in real-life situations and how different linguistic features contribute to the overall communicative function.\n\nOverall, the research on linguistic features and their functional associations provides insights into how language is structured and used in different contexts, helping us understand the complexities of human communication.","19":"these features can include parts of speech, sentence structure, word order, verb tense, and more. The purpose of automated grammar analysis is to assist in language learning, proofreading, and text analysis.\n\nComputer programs use various algorithms and techniques to analyze the grammar of a given text. One common approach is to use rule-based systems, where a set of predefined grammar rules is applied to the text. These rules can be based on linguistic theories or language-specific guidelines.\n\nAnother approach is statistical analysis, where the program uses machine learning algorithms to analyze large amounts of text data and learn patterns of grammar usage. This approach can be more flexible and adaptable to different languages and contexts.\n\nAutomated grammar analysis programs can provide feedback on grammar errors, suggest corrections, and highlight areas that need improvement. They can also generate detailed reports on the grammatical features of a text, such as the frequency of certain grammatical structures or the distribution of different parts of speech.\n\nHowever, it's important to note that automated grammar analysis is not perfect and may not always capture the nuances of language. Contextual understanding, idiomatic expressions, and stylistic choices are often challenging for computer programs to analyze accurately. Therefore, human proofreading and interpretation are still necessary for accurate and comprehensive language analysis.","20":"this process is known as text annotation. Text annotation involves the identification and labeling of various linguistic elements in a text, such as parts of speech, named entities, syntactic structures, sentiment, and other language features.\n\nTo automatically computer tag the entire corpus of texts, natural language processing (NLP) techniques are commonly employed. NLP algorithms analyze the text and assign appropriate tags or labels to different elements based on predefined rules or statistical models. These tags help in understanding the structure and meaning of the text.\n\nHowever, automatic tagging is not always perfect, and errors or inaccuracies can occur. Therefore, interactive editing is necessary to ensure the accuracy of the language features identified by the computer. Human annotators review and correct any mislabeled or missed elements, improving the quality of the annotations.\n\nThe interactive editing process may involve manually correcting tags, adding missing annotations, or removing incorrect ones. It requires linguistic expertise and a deep understanding of the language features being annotated. The goal is to create a high-quality annotated corpus that can be used for various language-related tasks, such as machine learning, information extraction, sentiment analysis, and more.\n\nOverall, the combination of automatic computer tagging and interactive editing ensures that language features are accurately identified in the corpus, providing a valuable resource for linguistic analysis and computational applications.","21":"these counts are then used to calculate various linguistic measures such as lexical diversity, syntactic complexity, and semantic similarity. The programs analyze the texts at different levels, including phonetics, morphology, syntax, and semantics, to capture a comprehensive understanding of the language features present in each text.\n\nThe normalized frequency counts provide a quantitative measure of how frequently each language feature occurs in a text relative to the total number of words or sentences. This allows researchers to compare and contrast the language features across different texts and identify patterns or trends.\n\nFor example, lexical diversity measures the variety of words used in a text. By calculating the normalized frequency counts of unique words in each text, researchers can determine the richness of vocabulary and the level of lexical diversity. This information can be useful in studying language development, language proficiency, or even authorship attribution.\n\nSyntactic complexity measures the complexity of sentence structures in a text. By analyzing the normalized frequency counts of different syntactic structures such as clauses, phrases, or sentence types, researchers can assess the syntactic complexity of a text. This information can be valuable in studying language acquisition, language processing, or even language disorders.\n\nSemantic similarity measures the similarity of meaning between words or sentences in a text. By computing the normalized frequency counts of semantic features such as word associations, semantic relations, or semantic categories, researchers can determine the semantic similarity between texts. This information can be applied in various fields, including natural language processing, information retrieval, or machine translation.\n\nOverall, the development and use of computer programs to compute normalized frequency counts of language features in a corpus enable researchers to conduct quantitative analyses and gain insights into various linguistic aspects. These programs provide a systematic and efficient way to analyze large amounts of text data and contribute to advancements in linguistics and related fields.","22":"this analysis helps to identify relationships between different linguistic features based on how often they occur together in a given dataset. Factor analysis is a statistical technique that aims to explain the variance in a set of variables by identifying underlying factors that contribute to their co-occurrence.\n\nIn the context of linguistics, factor analysis can be used to uncover patterns of co-occurrence between various linguistic features, such as phonetic, syntactic, or semantic features. For example, researchers may analyze a corpus of spoken language to identify patterns of co-occurrence between certain phonetic features, such as vowel quality or consonant clusters. By examining the frequency counts of these features, factor analysis can reveal underlying factors that explain their co-occurrence, such as regional dialects or speech styles.\n\nFactor analysis can also be applied to syntactic or semantic features to uncover patterns of co-occurrence between different grammatical structures or word meanings. For instance, researchers may analyze a corpus of written texts to identify patterns of co-occurrence between certain syntactic constructions, such as passive voice or relative clauses. By examining the frequency counts of these constructions, factor analysis can reveal underlying factors that explain their co-occurrence, such as genre or discourse context.\n\nOverall, factor analysis of frequency counts helps to uncover patterns of co-occurrence between linguistic features, providing insights into the underlying factors that contribute to their relationship. This analysis can be useful for understanding language variation, language change, or language processing, among other linguistic phenomena.","23":"they represent the latent variables that explain the observed correlations among a set of variables. These factors are not directly observable but are inferred from the patterns of correlations among the observed variables.\n\nFactor analysis is a statistical technique used to identify these underlying dimensions or factors. It helps in reducing the dimensionality of a dataset by grouping variables that are highly correlated with each other. By doing so, it provides a more concise representation of the data and helps in understanding the underlying structure or dimensions that drive the observed correlations.\n\nThe interpretation of these factors is subjective and depends on the context of the study. Researchers often assign labels or names to these factors based on their understanding of the variables involved. For example, in a study on personality traits, the factors may be interpreted as extraversion, neuroticism, agreeableness, etc.\n\nThe factors extracted through factor analysis can be used for various purposes, such as creating composite scores, developing scales or questionnaires, identifying underlying dimensions in psychological or social phenomena, and exploring the relationships between variables.\n\nOverall, the factors in factor analysis represent the underlying dimensions of variation that explain the observed correlations among a set of variables, and their interpretation is based on the context and understanding of the researcher.","24":"This analysis method involves calculating dimension scores for each text and then comparing the mean dimension scores for each record. The purpose is to identify significant linguistic similarities and differences between the records.\n\nTo perform this analysis, the following steps can be followed:\n\n1. Define the dimensions: Determine the specific linguistic dimensions that you want to analyze. These dimensions could include factors such as sentiment, formality, complexity, or any other relevant linguistic characteristic.\n\n2. Assign scores: Develop a scoring system for each dimension. This could involve rating each text on a scale (e.g., 1-5) for each dimension or using a more complex scoring method based on linguistic analysis techniques.\n\n3. Calculate dimension scores: Apply the scoring system to each text to calculate the dimension scores. This will result in a set of scores for each dimension for each text.\n\n4. Calculate mean dimension scores: For each record, calculate the mean dimension scores by averaging the scores across all texts within that record. This will provide a single value for each dimension for each record.\n\n5. Compare mean dimension scores: Compare the mean dimension scores across different records to identify salient linguistic similarities and differences. Look for patterns or significant deviations in the mean scores for each dimension.\n\n6. Interpret the results: Analyze the findings and interpret the results. Identify any consistent patterns or differences in the mean dimension scores that could indicate linguistic similarities or differences between the records.\n\nIt is important to note that the specific scoring system and dimensions used will depend on the research question or objective of the analysis. Additionally, statistical tests or visualizations can be used to further analyze and interpret the results, depending on the complexity of the data and research goals.","25":"focuses specifically on the analysis of lexical features in academic discourse. This approach aims to examine the role of vocabulary and word choice in shaping the discourse and conveying meaning.\n\nLexical AMD involves the identification and analysis of various lexical features, such as word frequency, word length, word complexity, and word connotation. By examining these features, researchers can gain insights into how specific lexical choices contribute to the overall discourse.\n\nFor example, the frequency of certain words or phrases can indicate their importance or relevance in a particular academic field. Word length and complexity can reflect the level of formality or technicality in the discourse. Additionally, word connotation can reveal the underlying attitudes or perspectives conveyed by the author.\n\nBy applying lexical AMD, researchers can uncover patterns and trends in academic discourse that may not be apparent through other analytical approaches. This method provides a more nuanced understanding of how vocabulary and word choice shape the communication of ideas in academic writing.\n\nOverall, lexical AMD offers a valuable perspective on academic discourse by focusing on the role of vocabulary and lexical features. It enhances our understanding of how language choices contribute to the construction of meaning and knowledge in academic contexts.","26":"In other words, while a traditional MD Analysis focuses on the grammatical aspects of a text and aims to identify how different records vary in terms of their function, a lexical MD Analysis focuses on the use of specific words, word combinations (n-grams), and collocations to identify these dimensions of variation.","27":"AMD, or Accumulated Mean Differences, is a statistical measure used in corpus linguistics to analyze language variation and change over time. It calculates the mean difference between two or more corpora at different time points, allowing researchers to identify linguistic patterns and trends.\n\nBerber Sardinha (2000) suggests that AMD is particularly useful for analyzing corpora that grow incrementally. This means that as new data is added to the corpus over time, AMD can be used to measure the linguistic changes that occur. By comparing the mean differences between different time points, researchers can track the evolution of language use and identify significant shifts or developments.\n\nThe cumulative nature of AMD makes it suitable for studying long-term language change, as it takes into account the entire corpus history. This allows researchers to capture the gradual linguistic transformations that occur over time, rather than focusing solely on individual time points.\n\nOverall, AMD provides a valuable tool for analyzing corpora that grow incrementally, enabling researchers to study language variation and change in a systematic and quantitative manner.","28":"\"a principled collection of authentic texts that are representative of a particular language or language variety, and that are stored in a computer-readable format for linguistic analysis.\"\n\nCorpus linguistics is a research approach that involves the analysis of large collections of texts, known as corpora, to investigate various linguistic phenomena. These corpora are carefully designed and prepared to ensure that they are representative of the language or language variety being studied.\n\nAccording to Sanchez (1995), a corpus is a collection of authentic texts that are selected based on specific criteria to ensure their representativeness. These texts can include written materials such as books, newspapers, and websites, as well as spoken materials such as transcriptions of conversations or speeches.\n\nThe corpus is stored in a computer-readable format, which allows for efficient and systematic analysis using computational tools and techniques. This format can vary depending on the specific research goals and requirements, but common formats include plain text files, XML files, or specialized corpus software formats.\n\nCareful corpus design and preparation are crucial in corpus linguistics research to ensure the reliability and validity of the findings. This involves selecting appropriate texts that cover a wide range of linguistic features and genres, as well as ensuring that the corpus is balanced and representative of the target language or language variety.\n\nAdditionally, the corpus should be annotated or tagged with linguistic information, such as part-of-speech tags or syntactic structures, to facilitate analysis and interpretation. This annotation process can be done manually or using automated tools, depending on the size and complexity of the corpus.\n\nOverall, corpus design and preparation are essential steps in corpus linguistics research, as they lay the foundation for accurate and meaningful analysis of linguistic phenomena.","29":"This definition refers to a corpus, which is a collection of linguistic data that is organized and processed by a computer for the purpose of description and analysis. The data can be in the form of oral or written language, or both, and should be extensive enough to represent the entirety or specific aspects of linguistic use. The corpus is systematized according to certain criteria and can provide various useful results for studying language.","30":"1. Clear research objectives: Before starting the corpus development process, it is essential to have a clear understanding of the research objectives. This includes identifying the specific linguistic phenomena or research questions that the corpus will address.\n\n2. Representative sample: The corpus should be representative of the target language or language variety. This means that it should include a diverse range of texts or language samples that accurately reflect the linguistic features, genres, and registers of the language being studied.\n\n3. Ethical considerations: Corpus development involves collecting and storing language data, which may raise ethical concerns. Researchers should ensure that they obtain informed consent from participants and follow ethical guidelines for data collection and storage.\n\n4. Annotation guidelines: Corpus annotation involves adding linguistic information to the corpus, such as part-of-speech tags or syntactic structures. It is important to establish clear annotation guidelines to ensure consistency and reliability in the annotation process. These guidelines should be well-documented and followed by all annotators involved in the corpus development.","31":"it should be composed of texts that are written or spoken by native speakers in their everyday communication. These texts can include various genres such as news articles, books, conversations, social media posts, emails, and more.\n\nThe requirement for authenticity ensures that the corpus reflects the natural usage of language, including its nuances, idiomatic expressions, and variations. It allows researchers to study and analyze language as it is actually used by native speakers in real-life situations.\n\nOn the other hand, texts specifically created for linguistic research or artificially generated by computer programs may not accurately represent the complexities and subtleties of natural language. They might lack the spontaneity, cultural references, and contextual information that are present in authentic texts.\n\nBy using authentic texts, researchers can gain insights into various linguistic phenomena, such as syntax, semantics, pragmatics, discourse analysis, and sociolinguistics. They can also investigate language variation across different regions, social groups, and time periods.\n\nOverall, the use of authentic texts in a corpus is crucial for linguistic research as it provides a reliable and representative source of language data for analysis and understanding.","32":"however, if the texts were produced by native speakers, the corpus would not necessarily be qualified as a corpus of learners.","33":"otherwise, the results and findings derived from the corpus may not be valid or reliable. \n\nTo ensure that the corpus is fit for purpose, several considerations need to be taken into account. \n\nFirstly, the corpus should be representative of the target language or domain. It should include a diverse range of texts that accurately reflect the language usage and characteristics of the research area. This can be achieved by selecting texts from various sources, such as books, articles, websites, and other relevant materials.\n\nSecondly, the corpus should be of sufficient size to provide reliable and meaningful results. The size of the corpus will depend on the research objectives and the specific research questions being investigated. Generally, a larger corpus is preferred as it allows for more robust statistical analysis and generalization of findings.\n\nThirdly, the corpus should be annotated or tagged appropriately to facilitate the analysis and extraction of relevant linguistic features. This may involve adding part-of-speech tags, syntactic annotations, semantic labels, or other linguistic annotations depending on the research requirements.\n\nFurthermore, the corpus should be well-documented, including information about its composition, sources, and any preprocessing steps that have been applied. This documentation is crucial for transparency and reproducibility, allowing other researchers to verify and build upon the findings.\n\nLastly, the corpus should be regularly updated and maintained to ensure its relevance and usefulness over time. Language usage and patterns evolve, and new texts and genres emerge, so it is important to periodically update the corpus to capture these changes.\n\nIn summary, constructing a fit-for-purpose corpus requires careful consideration of its representativeness, size, annotation, documentation, and maintenance. By adhering to these principles, researchers can ensure that their corpus is reliable, valid, and suitable for their specific research needs.","34":"A representative corpus is one that accurately reflects the language being studied in terms of its various linguistic features, such as vocabulary, grammar, and discourse patterns. It should cover a wide range of text types, genres, and registers to capture the diversity of language usage.\n\nThe size of a corpus plays a crucial role in its representativeness. A larger corpus is more likely to include a greater number of occurrences of low-frequency linguistic features. These features may be specific words, phrases, or grammatical constructions that are not commonly used but are still important for understanding the language.\n\nFor example, if a corpus is too small, it may not include enough instances of rare words or specialized terminology that are crucial for certain domains or fields of study. This can lead to an incomplete understanding of the language and its usage in specific contexts.\n\nOn the other hand, a larger corpus has a higher chance of including a sufficient number of occurrences of these low-frequency features. This allows researchers to analyze and study them more accurately, leading to a more comprehensive understanding of the language.\n\nHowever, it is important to note that the size of a corpus alone does not guarantee its representativeness. Other factors, such as the selection criteria for texts and the balance of different text types, also play a significant role. Therefore, researchers need to carefully consider these factors when creating or selecting a corpus to ensure its representativeness.","35":"Yes, according to Berber Sardinha (2018, p. 345), it is the responsibility of the researcher to demonstrate the representativeness and adequacy of the corpus they are using for their study. This means that the researcher should provide evidence or justification for why the chosen corpus is suitable for their research objectives and questions. They should explain how the corpus was compiled, what criteria were used for inclusion or exclusion of texts, and how it reflects the target population or phenomenon under investigation. By doing so, the researcher ensures transparency and allows for the evaluation and replication of their study by others.","36":"1. Number of texts: The size of a corpus can be determined by the number of texts it contains. This refers to the total count of individual documents or pieces of text that are included in the corpus. The more texts a corpus has, the larger its size.\n\n2. Text length: Another dimension that defines the size of a corpus is the length of the texts within it. This refers to the number of words, sentences, or characters in each text. A corpus with longer texts will generally be considered larger than one with shorter texts, even if they have the same number of documents.\n\n3. Text diversity: The diversity of texts in a corpus also contributes to its size. This dimension refers to the variety of genres, topics, or sources represented in the corpus. A corpus that includes texts from a wide range of domains or genres will be considered larger in terms of its coverage and representativeness.\n\nThese three dimensions collectively determine the size of a corpus and can be used to compare and classify different corpora based on their scale and scope.","37":"There are approximately 171,476 words in the English language, according to the Oxford English Dictionary. However, this number is constantly changing as new words are added and old words become obsolete.","38":"I'm sorry, but I cannot provide the number of texts as I am an AI language model and do not have access to personal data or information.","39":"I'm sorry, but I need more information to provide an accurate answer. Number of records in what context? Could you please provide more details?","40":"In this section, we will outline the specific steps and procedures that will be followed to conduct the study. The methodology will be described in a clear and concise manner to ensure transparency and replicability of the research.\n\n1. Research Design: The research design will be determined based on the objectives and research questions of the study. It could be experimental, observational, qualitative, or quantitative, depending on the nature of the research.\n\n2. Sampling: The target population will be identified, and a suitable sampling technique will be employed to select the participants for the study. The sample size will be determined based on statistical considerations and the research design.\n\n3. Data Collection: The data collection methods will be described in detail. This may include surveys, interviews, observations, experiments, or a combination of these methods. The instruments or tools used for data collection will be specified, along with their validity and reliability measures.\n\n4. Data Analysis: The procedures for data analysis will be outlined. This may involve statistical techniques, content analysis, thematic analysis, or other appropriate methods depending on the nature of the data collected. The software or tools used for data analysis will also be mentioned.\n\n5. Ethical Considerations: Any ethical considerations related to the study will be addressed. This may include obtaining informed consent from participants, ensuring confidentiality and anonymity, and following ethical guidelines set by relevant institutions or organizations.\n\n6. Limitations: The potential limitations of the study will be acknowledged. This may include constraints related to sample size, data collection methods, or external factors that may impact the validity or generalizability of the findings.\n\n7. Validity and Reliability: The steps taken to ensure the validity and reliability of the study will be discussed. This may include pilot testing of instruments, inter-rater reliability checks, or triangulation of data from multiple sources.\n\n8. Timeline: A timeline will be provided to outline the expected duration of each phase of the study, including data collection, analysis, and report writing.\n\nBy detailing the methodological procedures, this section aims to provide a clear roadmap for the study, ensuring that the research is conducted in a systematic and rigorous manner.","41":"The hypothesis for the study is as follows: The use of an AI-based tool (ChatGPT or Google Bard) to review works by Brazilian researchers using ERPP in the field of Applied Linguistics will result in a greater magnitude of record variation compared to the application of the same AI-based tool on articles from the same field that were accepted by reviewers of the main journals publishing in LI.","42":"In other words, the relevance of the AI-based tool can be determined by comparing the registration variation it provides to the registration variation obtained in a work composed in ERPP (Electronic Research Publishing Platform) that was accepted by a reputable journal in the field of LI (Language and Linguistics).\n\nThis statement suggests that the AI-based tool's effectiveness can be evaluated by measuring its ability to accurately register or record data compared to the registration variation observed in a high-quality research paper published in a respected journal. If the tool can achieve similar or better registration variation, it would indicate its relevance and usefulness in the field of LI research.\n\nHowever, it is important to note that the statement does not provide specific details about the AI-based tool or the nature of the registration variation being discussed. Therefore, a more comprehensive understanding of the context and specific metrics used for evaluation is necessary to fully assess the relevance of the AI-based tool.","43":"1. Literature review: Conduct a comprehensive review of existing research and literature related to the topic of the research project. This will help in identifying gaps in knowledge and understanding, as well as providing a foundation for the research.\n\n2. Research design: Develop a research design that outlines the overall approach, methodology, and data collection methods to be used in the project. This will include determining the research questions or hypotheses, selecting the appropriate research methods, and identifying the target population or sample.\n\n3. Data collection: Collect relevant data using various methods such as surveys, interviews, observations, or experiments. Ensure that the data collection process is ethical and follows appropriate protocols. This may involve obtaining informed consent from participants, maintaining confidentiality, and ensuring data accuracy and reliability.\n\n4. Data analysis: Analyze the collected data using appropriate statistical or qualitative analysis techniques. This will involve organizing, coding, and interpreting the data to draw meaningful conclusions and answer the research questions or test the hypotheses.\n\n5. Results interpretation: Interpret the results of the data analysis in the context of the research objectives. Identify patterns, trends, or relationships in the data and discuss their implications. Compare the findings with existing literature and theories to provide a comprehensive understanding of the research topic.\n\n6. Discussion and conclusion: Summarize the main findings of the research project and discuss their significance. Address any limitations or challenges encountered during the research process and suggest areas for future research. Draw conclusions based on the research findings and provide recommendations for practical applications or further investigations.\n\n7. Report writing: Prepare a comprehensive research report that includes an introduction, literature review, research design, data collection and analysis, results interpretation, discussion, and conclusion. Follow appropriate formatting and citation guidelines to ensure the report is clear, concise, and accessible to the intended audience.\n\n8. Presentation and dissemination: Present the research findings to relevant stakeholders, such as academic conferences, industry professionals, or policymakers. Disseminate the research through publications in peer-reviewed journals, online platforms, or other appropriate channels to contribute to the existing knowledge base and promote further discussion and collaboration.","44":"AMD stands for Automatic Metadata Extraction in Dimensions, which is a tool developed by Digital Science. It is used to extract metadata from scholarly publications and analyze the dimensions of a corpus.\n\nTo prepare the corpus of Applied Linguistics Published Papers (ALPP), a collection of relevant papers from various sources such as academic journals, conference proceedings, and books will be gathered. These papers should cover a wide range of topics and research areas within the field of Applied Linguistics.\n\nOnce the corpus is compiled, it will be processed using the AMD tool. AMD uses natural language processing techniques to automatically extract metadata from the papers, including information such as author names, publication titles, abstracts, keywords, and citation data.\n\nAfter the metadata extraction, the dimensions of the ALPP corpus will be analyzed using the AMD tool. Dimensions refer to different aspects or categories that can be used to classify and organize the corpus. For example, dimensions could include research topics, methodologies, theoretical frameworks, or publication venues.\n\nThe AMD tool will analyze the extracted metadata to identify patterns and trends within the corpus. It can provide insights into the distribution of papers across different dimensions, the most common keywords or topics, the most influential authors or publications, and other relevant information.\n\nBy analyzing the dimensions of the ALPP corpus, researchers and practitioners in Applied Linguistics can gain a better understanding of the current state of the field, identify emerging trends, and explore potential research gaps. This analysis can also help in designing future studies and informing decision-making in the field of Applied Linguistics.","45":"the AI-ALPP corpus will be used to study various aspects of applied linguistics, such as language acquisition, language teaching, and language assessment. The AMD analysis will provide insights into the size, diversity, and distribution of the corpus, allowing researchers to understand the scope and representativeness of the data.\n\nAdditionally, the AI-based tool used for proofreading will help improve the quality and accuracy of the texts in the corpus. It will identify and correct grammatical errors, spelling mistakes, and other language-related issues, ensuring that the texts are of high quality and suitable for research purposes.\n\nThe creation of the AI-ALPP corpus will greatly benefit the field of applied linguistics. Researchers will have access to a large and reliable dataset that can be used to investigate various research questions and hypotheses. The AI-based tool will also save time and effort in the proofreading process, allowing researchers to focus more on the analysis and interpretation of the data.\n\nOverall, the AI-ALPP corpus and the use of an AI-based tool for proofreading will contribute to the advancement of applied linguistics research and facilitate the development of innovative approaches and methodologies in the field.","46":"The magnitude of the registration variation verified in steps 1 and 2 can be referred to as \"AI-Assisted Variation - Published Reference - AI-VPR\".","47":"AMD stands for Automatic Metadata Extraction in Documents, which is a technique used to extract metadata from a corpus of documents. In the case of the Applied Linguistics Brazilian-authored Papers (ALBP) corpus, AMD will be used to analyze the dimensions of the corpus.\n\nThe dimensions of a corpus refer to various aspects such as the number of documents, the length of each document, the topics covered, the authors' affiliations, the publication dates, and other relevant metadata. By analyzing the dimensions of the ALBP corpus, researchers can gain insights into the characteristics and scope of the corpus.\n\nTo apply AMD to the ALBP corpus, the following steps will be taken:\n\n1. Data collection: The corpus will be collected by gathering Applied Linguistics works written by Brazilian authors. This can be done by searching academic databases, online repositories, and journals that publish research in the field of Applied Linguistics.\n\n2. Preprocessing: The collected documents will undergo preprocessing steps such as removing irrelevant content (e.g., advertisements, footnotes), standardizing the format (e.g., converting PDFs to plain text), and ensuring the documents are in a machine-readable format.\n\n3. Metadata extraction: AMD techniques will be applied to extract metadata from the corpus. This can include extracting information such as the title, author names, abstract, keywords, publication date, journal\/conference name, and author affiliations.\n\n4. Dimension analysis: The extracted metadata will be analyzed to determine the dimensions of the ALBP corpus. This can involve counting the number of documents, calculating the average length of documents, identifying the most frequent topics or keywords, analyzing the distribution of publication dates, and examining the affiliations of the authors.\n\n5. Visualization and interpretation: The analyzed dimensions can be visualized using graphs, charts, or other visual representations. This will help researchers gain a better understanding of the characteristics and trends within the ALBP corpus. The dimensions can also be interpreted to draw conclusions about the research landscape in Applied Linguistics by Brazilian authors.\n\nBy applying AMD to the ALBP corpus, researchers can gain valuable insights into the characteristics and scope of Applied Linguistics works written by Brazilian authors. This analysis can inform future research directions, identify research gaps, and contribute to the advancement of the field.","48":"the AI-ALBP corpus will be used to investigate the linguistic features and patterns in applied linguistics papers written by Brazilian authors. The AMD (Analytical Multidimensional Description) method will be employed to analyze the dimensions of the corpus, which includes various linguistic variables such as vocabulary richness, syntactic complexity, and discourse organization. This analysis will provide insights into the characteristics of AI-reviewed papers in the field of applied linguistics written by Brazilian authors, shedding light on the impact of AI tools on academic writing.","49":"The magnitude of the registration variation verified in steps 4 and 5 can be referred to as \"AI-Assisted Variation - Brazilian-authored Reference - AI-VBR\".","50":"AI-VPR (Artificial Intelligence-Video Person Re-identification) and AI-VBR (Artificial Intelligence-Video-based Re-identification) are two technologies used for interpreting and analyzing video data in the context of person re-identification.\n\nAI-VPR focuses on identifying and tracking individuals across multiple cameras or video frames. It uses deep learning algorithms to extract features from video frames and match them with existing person identities in a database. The interpretation of AI-VPR results involves analyzing the accuracy of person matching, tracking performance, and potential errors or false positives\/negatives. It also involves evaluating the efficiency and scalability of the AI-VPR system in handling large-scale video data.\n\nAI-VBR, on the other hand, is a broader term that encompasses various video-based re-identification techniques, including person, object, or vehicle re-identification. It involves analyzing video data to identify and track specific entities across different cameras or video frames. The interpretation of AI-VBR results includes evaluating the accuracy of re-identification, tracking performance, and potential challenges such as occlusion, lighting changes, or pose variations. It also involves assessing the effectiveness of AI-VBR algorithms in handling different types of entities and scenarios.\n\nIn both cases, the interpretation of results involves analyzing various metrics such as precision, recall, accuracy, and F1 score to assess the performance of AI-VPR or AI-VBR systems. It also involves considering real-world factors such as computational resources, scalability, and deployment feasibility. Additionally, the interpretation of results may involve comparing the performance of AI-VPR or AI-VBR systems with other existing methods or benchmarks to evaluate their effectiveness and advancements in the field.","51":"To design the four corpora according to the high-level requirements described in Table 1, the following steps can be followed:\n\n1. Understand the high-level requirements: Carefully read and comprehend the requirements mentioned in Table 1. Identify the key elements and objectives that need to be fulfilled in each corpus.\n\n2. Determine the purpose of each corpus: Based on the requirements, determine the specific purpose of each corpus. For example, if one corpus is intended for sentiment analysis, another for named entity recognition, and so on.\n\n3. Define the data sources: Identify the data sources that will be used to create each corpus. This could include various types of text data such as news articles, social media posts, academic papers, or any other relevant sources.\n\n4. Gather and preprocess the data: Collect the necessary data from the identified sources. Preprocess the data to ensure it is clean, consistent, and in the desired format. This may involve removing duplicates, correcting errors, standardizing text, and other necessary preprocessing steps.\n\n5. Annotate the data: Based on the requirements, annotate the collected data with the relevant labels or annotations. This could involve manual annotation by human annotators or using automated annotation techniques depending on the nature of the requirements.\n\n6. Split the data into appropriate subsets: Divide the annotated data into appropriate subsets based on the requirements. This could include training, validation, and testing sets, or any other specific subsets as needed.\n\n7. Validate the corpus: Validate the created corpus to ensure it meets the high-level requirements. This may involve conducting quality checks, evaluating the annotation accuracy, and making any necessary adjustments or improvements.\n\n8. Document the corpus: Create documentation for each corpus, including details about the data sources, annotation guidelines, annotation process, and any other relevant information. This documentation will help users understand and utilize the corpus effectively.\n\n9. Make the corpora available: Publish or make the corpora available to the intended users or researchers. This could involve sharing the corpora through a public repository, providing access through an API, or any other suitable means.\n\nBy following these steps, the four corpora can be designed and created according to the high-level requirements mentioned in Table 1.","52":"This statement suggests that for the purpose of the study, texts produced by ERPP (Enterprise Resource Planning and Production) user authors with the help of AI-based tools will be considered authentic, even though they are not produced by human authors without any assistance from computer programs.\n\nThe use of AI-based tools in assisting authors in generating texts is becoming increasingly common. These tools can help with tasks such as grammar and spell checking, suggesting alternative word choices, or even generating entire sentences or paragraphs based on given prompts. While these texts may not be entirely produced by human authors, they are still considered authentic for the study's purposes.\n\nThe decision to consider these texts as authentic may be based on the assumption that the AI-based tools are merely assisting the authors and not completely generating the content. The authors are still actively involved in the writing process and have control over the final output. Therefore, the texts are seen as representative of natural language usage and suitable for inclusion in the corpus.\n\nIt is important to note that this decision may vary depending on the specific goals and requirements of the study. In some cases, researchers may choose to exclude texts generated with the assistance of AI-based tools to maintain a more traditional definition of authenticity. However, in this particular study, the inclusion of such texts is deemed acceptable.","53":"The ABPP (Adequate and Balanced Portuguese-Portuguese) and ALBP (Adequate and Balanced Luso-Brazilian Portuguese) corpora will be created with the aim of achieving a high level of representativeness and adequacy. To ensure this, specific criteria will be defined and validated in consultation with the advisor.\n\nThe criteria for representativeness will involve selecting a diverse range of texts from various genres, domains, and registers. This will include texts from different sources such as newspapers, books, academic articles, websites, and social media platforms. The texts will cover a wide range of topics, ensuring that the corpora represent the linguistic diversity found in Portuguese-speaking communities.\n\nIn terms of adequacy, the corpora will be designed to accurately reflect the linguistic features and variations of both Portuguese-Portuguese and Luso-Brazilian Portuguese. This will involve including texts from different regions and social groups, capturing the linguistic nuances and variations specific to each variety of Portuguese.\n\nThe criteria for representativeness and adequacy will be defined and refined in collaboration with the advisor, who will provide expertise and guidance in ensuring that the corpora meet the required standards. The advisor's input will be crucial in determining the appropriate size, composition, and balance of the corpora.\n\nOverall, the ABPP and ALBP corpora will be meticulously originated and designed to meet the highest standards of representativeness and adequacy, ensuring their usefulness and reliability for linguistic research and analysis.","54":"This incremental approach to building the ALBP corpus from the second half of 2023 to the second half of 2024 can be a feasible strategy. By involving a group of 20 researchers using ERPP per semester, it allows for a steady and consistent growth of the corpus over time.\n\nThe estimated final word count of 162,000 words seems reasonable, considering an average paper length of 1,350 words. However, it is important to note that the representativeness and adequacy of the corpus should be taken into consideration.\n\nTo ensure representativeness, it is crucial to have a diverse range of topics and research areas covered by the papers. This can be achieved by encouraging researchers to explore different aspects of the ALBP domain and providing guidelines for topic selection.\n\nAdditionally, it is important to consider the adequacy of the corpus in terms of linguistic variation and genre diversity. Researchers should be encouraged to write papers in different styles and genres, such as research articles, case studies, literature reviews, etc. This will help capture a wider range of linguistic features and ensure the corpus reflects the diversity of academic writing in the ALBP domain.\n\nRegular evaluations and feedback sessions with the researchers can also help in refining the strategy and ensuring that the corpus meets the desired criteria of representativeness and adequacy. Adjustments can be made based on the feedback received, such as encouraging researchers to focus on specific areas that may be underrepresented or exploring new research directions.\n\nOverall, an incremental approach to building the ALBP corpus with the involvement of a group of researchers can be an effective strategy. However, it is important to continuously evaluate and refine the strategy to ensure the resulting corpus is representative and adequate for its intended purposes.","55":"To validate the premise of employing traditional (functional) AMD in the research project, several steps can be taken:\n\n1. Literature review: Conduct a thorough review of existing literature on traditional (functional) AMD to understand its principles, methodologies, and applications. This will help establish the foundation for the research project and validate the premise.\n\n2. Consultation with experts: Seek advice and guidance from experts in the field of traditional (functional) AMD. Discuss the feasibility and relevance of employing this approach in the specific research project. Their insights and expertise can help validate the premise.\n\n3. Pilot study: Conduct a small-scale pilot study using traditional (functional) AMD methods. This will provide an opportunity to test the effectiveness and suitability of this approach for the research project. The results of the pilot study can help validate the premise and inform any necessary modifications or adjustments.\n\n4. Comparison with alternative approaches: Compare traditional (functional) AMD with other available approaches or methodologies. Evaluate the strengths, weaknesses, and potential limitations of each approach. This comparative analysis can help validate the premise by demonstrating the unique advantages of traditional (functional) AMD for the research project.\n\n5. Peer review and feedback: Share the research project proposal with peers and experts in the field for review and feedback. Their input can help validate the premise by providing constructive criticism, suggestions, and insights. Incorporating their feedback will strengthen the overall validity of employing traditional (functional) AMD.\n\nBy following these steps, the premise of employing traditional (functional) AMD can be thoroughly validated, ensuring that the research project is based on a solid foundation and methodology.","56":"By utilizing data visualization technologies, this research project aims to enhance the understanding of complex relationships between variables. Traditional methods of presenting research data, such as tables and charts, often fail to capture the intricate connections between different variables. However, computer-generated visual representations can provide a more intuitive and comprehensive view of the data.\n\nThrough data visualization, researchers can identify patterns, trends, and correlations that may not be immediately apparent in raw data. This can lead to deeper insights and a better understanding of the research findings. By visually representing the relationships between variables, researchers can also communicate their results more effectively to a wider audience, including policymakers, stakeholders, and the general public.\n\nData visualization technologies offer various tools and techniques to represent data in visually appealing and interactive formats. These may include graphs, charts, maps, diagrams, and other visual elements. Researchers can choose the most appropriate visualization method based on the nature of their data and the research objectives.\n\nThe benefits of using data visualization in research are numerous. Firstly, it allows for a more efficient analysis of complex data sets, as visual representations can highlight important patterns and outliers. Secondly, it enables researchers to explore different variables and their relationships in a dynamic and interactive manner, facilitating a deeper understanding of the research topic. Thirdly, data visualization can enhance the communication of research findings, making them more accessible and engaging to a wider audience.\n\nIn summary, the utilization of data visualization technologies in this research project will enable the assessment of complex relationships between variables and facilitate the dissemination of research data. By visually representing the results, researchers can gain deeper insights, communicate their findings effectively, and engage a broader audience in the research process.","57":"Data visualization is the graphical representation of data and information using visual elements such as charts, graphs, and maps. It helps to present complex data in a more understandable and visually appealing way, allowing users to easily interpret and analyze the information.\n\nOpen source libraries like D3.js, ECharts, Vega, and deck.gl provide tools and frameworks for creating interactive and dynamic data visualizations. These libraries offer a wide range of pre-built visualization components and functionalities, making it easier for developers to create visually appealing and interactive data visualizations.\n\nBy utilizing these open source libraries, developers can leverage the power of these tools to create custom data visualizations that suit their specific needs. These libraries often provide a rich set of features, including support for various chart types, data manipulation, animation, and interactivity, enabling developers to create highly interactive and engaging data visualizations.","58":"In the next section, the research project will be described in detail, including the methodology, data collection methods, and analysis techniques to be used. This section will outline the step-by-step process of conducting the research and provide a clear roadmap for the project.\n\nThe methodology will explain the overall approach to be taken in the research, such as whether it will be qualitative, quantitative, or a combination of both. It will also describe the research design, including any experimental or observational methods to be employed.\n\nThe data collection methods will be outlined, specifying how the necessary data will be gathered. This may include surveys, interviews, observations, experiments, or the collection of existing data from various sources. The section will also explain the sampling strategy, including the target population and the sample size.\n\nThe analysis techniques to be used will be detailed, including any statistical or qualitative analysis methods. This may involve the use of software or tools for data analysis, such as SPSS or NVivo. The section will also explain how the data will be interpreted and presented, including any visualizations or tables that will be used.\n\nAdditionally, this section may include a timeline or schedule for the research project, outlining the key milestones and deadlines. It may also discuss any potential limitations or challenges that may arise during the research and how they will be addressed.\n\nOverall, this section will provide a comprehensive overview of how the research project will be carried out, ensuring that the research is conducted in a systematic and rigorous manner.","59":"The corpora design is an essential first task for the project, as it lays the foundation for the subsequent stages. It involves determining the scope and structure of the corpora, which will serve as the primary source of data for the project.\n\nDuring the initial validation phase with the advisor, the project team will discuss and finalize the specific requirements and objectives of the corpora design. This may include identifying the target languages, domains, and genres to be included in the corpora.\n\nOnce the requirements are defined, the team can proceed with collecting and curating the necessary data. This may involve sourcing existing corpora from reputable linguistic resources, as well as creating new corpora through data collection methods such as web scraping or manual annotation.\n\nThe corpora design task also encompasses decisions regarding data preprocessing and annotation. This may involve cleaning and normalizing the data, as well as annotating it with relevant linguistic information such as part-of-speech tags or syntactic structures.\n\nAdditionally, the team will need to consider ethical considerations and ensure compliance with data protection regulations during the corpora design process. This may involve obtaining consent from data sources or anonymizing sensitive information.\n\nBy completing the corpora design task in the second half of 2023, the project will have a well-defined and curated dataset that can be used for subsequent stages such as model training and evaluation.","60":"The ALBP corpus, which stands for \"Artificial Language Benchmarking Project\" corpus, is a linguistic resource that will be created for benchmarking and evaluating artificial language models. Initially, the corpus will be populated with texts that are generated in the latter half of 2023. These texts will serve as the foundation for the corpus.\n\nHowever, as the project advances and more data becomes available, the ALBP corpus is anticipated to expand gradually. This growth will involve the addition of new texts, which will contribute to the diversity and richness of the corpus. The continuous expansion of the corpus will enable researchers and developers to have access to a larger and more comprehensive dataset for training and evaluating artificial language models.","61":"Figure 2: Project Timeline\n\n1. Initial script development: The necessary computer scripts will be developed in parallel with the project initiation. This will involve creating the basic framework for data analysis and visualization.\n\n2. Multidimensional analysis: Once the initial scripts are ready, multidimensional analyzes will be performed. This will involve analyzing data from multiple dimensions, such as time, location, and various variables of interest.\n\n3. Refinement of analysis: As the project progresses, the initial analysis results will be refined. This may involve adjusting parameters, incorporating additional data sources, or improving the accuracy of the analysis.\n\n4. Expansion of analysis: Based on the refined analysis, the scope of the analysis will be expanded. This may involve exploring new dimensions, incorporating additional variables, or conducting more detailed analyses.\n\n5. Iterative process: The process of refining and expanding the analysis will be iterative, with each iteration building upon the previous one. This will allow for continuous improvement and optimization of the analysis.\n\nBy following this timeline and iterative approach, the necessary computer scripts will be prepared, and the multidimensional analysis will be performed, refined, and expanded as the project progresses. This will ensure that the analysis is comprehensive, accurate, and aligned with the project objectives.","62":"The completion of the project and the presentation of the final revision of the dissertation are anticipated to occur by the conclusion of the first semester of 2025."}}